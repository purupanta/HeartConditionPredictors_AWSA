{"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","from imblearn.over_sampling import SMOTENC\n","from imblearn.combine import SMOTEENN\n","from imblearn.over_sampling import BorderlineSMOTE\n","from imblearn.over_sampling import SMOTE\n","from imblearn.over_sampling import ADASYN"],"metadata":{"id":"XcI1aavOy-ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxKjoTJl8LvO"},"outputs":[],"source":["# Mounting google drive if it is already not mounted\n","def load_data(verbose_level, ip_file_path, ip_sheet_name):\n","  googleDriveFolder = '/content/drive'\n","\n","  # Loading cleaded excel data\n","  df_orig = pd.read_excel(ip_file_path, sheet_name=ip_sheet_name)\n","\n","  if verbose_level > 0:\n","    print(\"Ip File Path: \" + ip_file_path + \"\\n SheetName: \" + ip_sheet_name)\n","    print(f\"df_orig.shape: {df_orig.shape}\")\n","  if verbose_level > 1:\n","    display(df_orig.head())\n","\n","  return df_orig\n","\n","\n","# SMOTE type2: Apply SMOTEENN only to training data\n","def ApplySMOTEENN(X_train, y_train):\n","    sm = SMOTEENN(random_state=42)\n","    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n","    return X_train_res, y_train_res\n","\n","# Main preprocessing function\n","def data_preprocessing(verbose_level, X, y):\n","    # -------------------------------------\n","    # STEP 1: Train / Validation / Test Split (60/20/20)\n","    # -------------------------------------\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","        X, y, test_size=0.4, stratify=y, random_state=42\n","    )\n","\n","    X_val, X_test, y_val, y_test = train_test_split(\n","        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n","    )\n","\n","    # -------------------------------------\n","    # STEP 2: Apply SMOTEENN on training data only\n","    # -------------------------------------\n","    X_train_res, y_train_res = ApplyADASYN(X_train.copy(), y_train.copy())\n","    X_train_res = pd.DataFrame(X_train_res, columns=X.columns)\n","\n","    # -------------------------------------\n","    # STEP 3: Scale features\n","    # -------------------------------------\n","    scaler = StandardScaler()\n","    X_train_res_scaled = scaler.fit_transform(X_train_res)\n","    X_val_scaled = scaler.transform(X_val)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # Convert to DataFrames\n","    X_train_res_scaled = pd.DataFrame(X_train_res_scaled, columns=X.columns)\n","    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n","    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n","\n","    # -------------------------------------\n","    # STEP 4: Verbose summary\n","    # -------------------------------------\n","    if verbose_level > 0:\n","      print(f\"\\nOriginal data shape: {X.shape}\")\n","\n","      print(f\"\\nBefore SMOTE => (Train shape: {X_train.shape}), \"\n","            f\"(Validation shape: {X_val.shape}), (Test shape: {X_test.shape})\")\n","\n","      print(f\"After SMOTE  => (Train shape: {X_train_res.shape}), \"\n","            f\"(Validation shape: {X_val.shape}), (Test shape: {X_test.shape})\")\n","\n","      print(f\"After Scaling => (Train shape: {X_train_res_scaled.shape}), \"\n","            f\"(Validation shape: {X_val_scaled.shape}), (Test shape: {X_test_scaled.shape})\")\n","\n","\n","    # -------------------------------------\n","    # STEP 5: Return data\n","    # -------------------------------------\n","    return {\n","        \"X_train\": X_train,\n","        \"y_train\": y_train,\n","        \"X_val\": X_val,\n","        \"y_val\": y_val,\n","        \"X_test\": X_test,\n","        \"y_test\": y_test,\n","\n","        \"X_train_res\": X_train_res,\n","        \"y_train_res\": y_train_res,\n","\n","        \"X_train_res_scaled\": X_train_res_scaled,\n","        \"X_val_scaled\": X_val_scaled,\n","        \"X_test_scaled\": X_test_scaled,\n","\n","        \"features\": X.columns.tolist()\n","    }\n","\n","\n","# SMOTE type1: Apply SMOTE only to training data\n","def ApplySMOTE(X_train, y_train):\n","  sm = SMOTE(random_state=42)\n","  X_res, y_res = sm.fit_resample(X_train, y_train)\n","  return X_res, y_res\n","\n","# SMOTE type2: Apply SMOTEENN only to training data\n","def ApplySMOTEENN(X_train, y_train):\n","  sm = SMOTEENN(random_state=42)\n","  X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n","  return X_train_res, y_train_res\n","\n","# SMOTE type3: Apply ApplyADASYN only to training data\n","def ApplyADASYN(X_train, y_train):\n","  adasyn = ADASYN(random_state=42)\n","  X_res, y_res = adasyn.fit_resample(X_train, y_train)\n","  return X_res, y_res\n","\n","# SMOTE type4: Apply ApplyBorderlineSMOTE only to training data\n","def ApplyBorderlineSMOTE(X_train, y_train, kind='borderline-1'):\n","  sm = BorderlineSMOTE(kind=kind, random_state=42)\n","  X_res, y_res = sm.fit_resample(X_train, y_train)\n","  return X_res, y_res\n","\n","# SMOTE type1: Apply SMOTENC only to training data\n","def ApplySMOTENC(X_train, y_train, cat_indices):\n","  sm = SMOTENC(categorical_features=cat_indices, random_state=42)\n","  X_res, y_res = sm.fit_resample(X_train, y_train)\n","  return X_res, y_res\n"]},{"cell_type":"code","source":["def export_merged_importance_matrics(importance_df_lr, importance_df_tabnet, importance_df_xgb):\n","  import pandas as pd\n","  import os\n","  from openpyxl import load_workbook\n","  from pandas import ExcelWriter\n","  # Step 1: Rename columns based on actual names\n","  importance_df_lr_renamed = importance_df_lr.rename(columns={\n","      'mean_shap': 'mean_shap_lr',\n","      'mean_abs_shap': 'mean_abs_shap_lr'\n","  })\n","\n","  # Do the same for the others — print their columns first if unsure\n","  importance_df_tabnet_renamed = importance_df_tabnet.rename(columns={\n","      'mean_shap': 'mean_shap_tabnet',\n","      'mean_abs_shap': 'mean_abs_shap_tabnet'\n","  })\n","\n","  importance_df_xgb_renamed = importance_df_xgb.rename(columns={\n","      'mean_shap': 'mean_shap_xgb',\n","      'mean_abs_shap': 'mean_abs_shap_xgb'\n","  })\n","\n","  # Step 2: Convert to numeric\n","  for df, cols in [\n","      (importance_df_lr_renamed, ['mean_shap_lr', 'mean_abs_shap_lr']),\n","      (importance_df_tabnet_renamed, ['mean_shap_tabnet', 'mean_abs_shap_tabnet']),\n","      (importance_df_xgb_renamed, ['mean_shap_xgb', 'mean_abs_shap_xgb'])\n","  ]:\n","      for col in cols:\n","          df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","  # Step 3: Merge\n","  merged_df = importance_df_lr_renamed.merge(\n","      importance_df_tabnet_renamed, on='feature', how='outer'\n","  ).merge(\n","      importance_df_xgb_renamed, on='feature', how='outer'\n","  )\n","\n","  # Step 4: Round all numeric columns\n","  # merged_df = merged_df.round(4)\n","\n","  # Step 5: Reorder columns\n","  merged_importance_df = merged_df[[\n","      'feature',\n","      'mean_shap_lr', 'mean_abs_shap_lr',\n","      'mean_shap_tabnet', 'mean_abs_shap_tabnet',\n","      'mean_shap_xgb', 'mean_abs_shap_xgb'\n","  ]]\n","\n","  # Step 6: Export to Excel (overwrite or update sheet)\n","  output_path = \"../op_data/merged_importance_matrics.xlsx\"\n","  sheet_name = \"importance_matrix_all\"\n","\n","  # Optionally: ensure directory exists\n","  os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","  with ExcelWriter(output_path, engine='openpyxl', mode='w') as writer:\n","      merged_importance_df.to_excel(writer, sheet_name=sheet_name, index=False)\n","\n","  print(f\"✅ Exported sheet '{sheet_name}' to: {output_path}\")\n"],"metadata":{"id":"TnOlTtJPynUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model_metric_summary_df(results_all: dict[str, dict]) -> pd.DataFrame:\n","    summary_rows = []\n","\n","    metrics = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n","    label_mapping = {\n","        \"0\": \"Class0\",\n","        \"1\": \"Class1\",\n","        \"macro avg\": \"MacroAvg\",\n","        \"weighted avg\": \"WeightedAvg\"\n","    }\n","\n","    for model_name, result in results_all.items():\n","        report = result.get(\"report_df\")\n","        if not isinstance(report, pd.DataFrame):\n","            raise TypeError(f\"'report_df' for model '{model_name}' must be a pandas DataFrame.\")\n","\n","        best_f1_thres = result.get(\"best_threshold\", np.nan)\n","        accuracy = result.get(\"accuracy\", np.nan)\n","        auc_roc = result.get(\"auc\", np.nan)\n","\n","        row = {\n","            \"ModelName\": model_name,\n","            \"Best_F1_Thres\": best_f1_thres,\n","            \"Accuracy\": accuracy,\n","            \"AUC-ROC\": auc_roc,\n","        }\n","\n","        for label, name in label_mapping.items():\n","            for metric in metrics:\n","                colname = f\"{metric.capitalize()}_{name}\"\n","                value = report.loc[label, metric] if label in report.index else np.nan\n","                row[colname] = value\n","\n","        summary_rows.append(row)\n","\n","    summary_df = pd.DataFrame(summary_rows)\n","\n","    column_order = [\n","        \"ModelName\", \"Best_F1_Thres\", \"Accuracy\", \"AUC-ROC\",\n","        \"Precision_Class0\", \"Precision_Class1\", \"Precision_MacroAvg\", \"Precision_WeightedAvg\",\n","        \"Recall_Class0\", \"Recall_Class1\", \"Recall_MacroAvg\", \"Recall_WeightedAvg\",\n","        \"F1-score_Class0\", \"F1-score_Class1\", \"F1-score_MacroAvg\", \"F1-score_WeightedAvg\",\n","        \"Support_Class0\", \"Support_Class1\", \"Support_MacroAvg\", \"Support_WeightedAvg\"\n","    ]\n","\n","    existing_columns = [col for col in column_order if col in summary_df.columns]\n","    summary_df = summary_df[existing_columns]\n","\n","    return summary_df"],"metadata":{"id":"-N2gSEp8yijC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDwKsYvI95/EJW8v8EeLHl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}