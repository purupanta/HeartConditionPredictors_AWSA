{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOGoAHkN5hL/oQPxgAgsvP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["SAINT"],"metadata":{"id":"iMTktPpSTW6g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import (\n","  accuracy_score, classification_report, roc_auc_score,\n","  f1_score, roc_curve\n",")"],"metadata":{"id":"9huOITRiVVLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_saint_pipeline(\n","  X_train, y_train,\n","  X_val, y_val,\n","  X_test, y_test,\n","  feature_names,\n","  device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","  embed_dim=32,\n","  num_heads=4,\n","  num_layers=2,\n","  pretrain_epochs=10,\n","  finetune_epochs=10,\n","  lr=1e-4\n","):\n","  # === Dataset Wrapper ===\n","  class TabularDataset_saint(Dataset):\n","    def __init__(self, X, y):\n","      if isinstance(X, pd.DataFrame): X = X.values\n","      if isinstance(y, pd.Series): y = y.values\n","      self.X = torch.tensor(X, dtype=torch.float32)\n","      self.y = torch.tensor(y, dtype=torch.long)\n","\n","    def __len__(self): return len(self.X)\n","    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n","\n","  # === SAINT Model ===\n","  class SAINT(nn.Module):\n","    def __init__(self, input_dim, embed_dim=32, num_heads=4, num_layers=2):\n","      super().__init__()\n","      self.embedding = nn.Linear(input_dim, embed_dim)\n","      encoder_layer = nn.TransformerEncoderLayer(\n","          d_model=embed_dim, nhead=num_heads, batch_first=True\n","      )\n","      self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","      self.classifier = nn.Linear(embed_dim, 2)\n","\n","    def forward(self, x):\n","      x = self.embedding(x).unsqueeze(1)\n","      x = self.transformer(x).squeeze(1)\n","      return self.classifier(x)\n","\n","    # === Contrastive Loss ===\n","  class NTXentLoss_saint(nn.Module):\n","    def __init__(self, temperature=0.5):\n","      super().__init__()\n","      self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","      z = torch.cat([z_i, z_j], dim=0)\n","      sim_matrix = torch.matmul(z, z.T) / self.temperature\n","      sim_exp = torch.exp(sim_matrix)\n","      mask = ~torch.eye(z.shape[0], dtype=torch.bool, device=z.device)\n","      sim_exp = sim_exp.masked_select(mask).view(z.shape[0], -1)\n","      positives = torch.exp(torch.sum(z_i * z_j, dim=-1) / self.temperature)\n","      positives = torch.cat([positives, positives], dim=0)\n","      loss = -torch.log(positives / sim_exp.sum(dim=1))\n","      return loss.mean()\n","\n","\n","  # === Initialize model\n","  input_dim = X_train.shape[1]\n","  model_saint = SAINT(input_dim, embed_dim, num_heads, num_layers).to(device)\n","\n","  # === Pretraining\n","  pretrain_loader = DataLoader(TabularDataset_saint(X_train, y_train), batch_size=128, shuffle=True)\n","  optimizer_pre = torch.optim.Adam(model_saint.parameters(), lr=lr)\n","  contrastive_loss = NTXentLoss_saint()\n","  model_saint.train()\n","  for epoch in range(pretrain_epochs):\n","      total_loss = 0\n","      for x_batch, _ in pretrain_loader:\n","          x_batch = x_batch.to(device)\n","          x_i = x_batch + torch.randn_like(x_batch) * 0.01\n","          x_j = x_batch + torch.randn_like(x_batch) * 0.01\n","          z_i = model_saint(x_i)\n","          z_j = model_saint(x_j)\n","          loss = contrastive_loss(z_i, z_j)\n","          optimizer_pre.zero_grad()\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model_saint.parameters(), 1.0)\n","          optimizer_pre.step()\n","          total_loss += loss.item()\n","      print(f\"[Pretrain] Epoch {epoch+1}: Loss = {total_loss / len(pretrain_loader):.4f}\")\n","\n","  # === Fine-tuning\n","  train_loader = DataLoader(TabularDataset_saint(X_train, y_train), batch_size=128, shuffle=True)\n","  optimizer_ft = torch.optim.Adam(model_saint.parameters(), lr=lr)\n","  model_saint.train()\n","  for epoch in range(finetune_epochs):\n","      total_loss = 0\n","      for x_batch, y_batch in train_loader:\n","          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","          logits = model_saint(x_batch)\n","          loss = nn.CrossEntropyLoss()(logits, y_batch)\n","          optimizer_ft.zero_grad()\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model_saint.parameters(), 1.0)\n","          optimizer_ft.step()\n","          total_loss += loss.item()\n","      print(f\"[Finetune] Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n","\n","  # === Predict Validation\n","  model_saint.eval()\n","  with torch.no_grad():\n","      x_val_tensor = torch.tensor(\n","          X_val.values if isinstance(X_val, pd.DataFrame) else X_val,\n","          dtype=torch.float32\n","      ).to(device)\n","      val_logits = model_saint(x_val_tensor)\n","      val_probs = torch.softmax(val_logits, dim=1)[:, 1].cpu().numpy()\n","\n","  # === Best threshold\n","  thresholds = np.linspace(0, 1, 101)\n","  f1s = [f1_score(y_val, (val_probs >= t).astype(int)) for t in thresholds]\n","  best_threshold = thresholds[np.argmax(f1s)]\n","  print(f\"\\nâœ… Best threshold (F1): {best_threshold:.2f}\")\n","\n","  # === Test Predictions\n","  with torch.no_grad():\n","      x_test_tensor = torch.tensor(\n","          X_test.values if isinstance(X_test, pd.DataFrame) else X_test,\n","          dtype=torch.float32\n","      ).to(device)\n","      test_logits = model_saint(x_test_tensor)\n","      test_probs = torch.softmax(test_logits, dim=1)[:, 1].cpu().numpy()\n","      test_preds = (test_probs >= best_threshold).astype(int)\n","\n","    # === Metrics\n","  accuracy = accuracy_score(y_test, test_preds)\n","  auc = roc_auc_score(y_test, test_probs)\n","  report_str = classification_report(y_test, test_preds, target_names=[\"Class 0\", \"Class 1\"])\n","  report_df = pd.DataFrame(classification_report(y_test, test_preds, output_dict=True)).T\n","\n","  print(f\"\\nðŸ”Ž Test Accuracy: {accuracy:.2f}\")\n","  print(\"Classification Report:\")\n","  print(report_str)\n","  print(f\"AUC-ROC (Test): {auc:.2f}\")\n","\n","  # === ROC Plot\n","  fpr, tpr, _ = roc_curve(y_test, test_probs)\n","  plt.figure(figsize=(6, 4))\n","  plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n","  plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n","  plt.xlabel(\"False Positive Rate\")\n","  plt.ylabel(\"True Positive Rate\")\n","  plt.title(\"ROC Curve - SAINT\")\n","  plt.legend()\n","  plt.grid(True)\n","  plt.tight_layout()\n","  plt.show()\n","\n","  # === Top 10 Feature Importance (placeholder for SAINT, or you can add attention weights later)\n","  feature_importance = pd.DataFrame({\n","      'Feature': feature_names,\n","      'Importance': np.random.dirichlet(np.ones(len(feature_names)), size=1)[0]  # Dummy values\n","  }).sort_values(by='Importance', ascending=False)\n","\n","  print(\"\\nðŸ“Š Top 10 Important Features (SAINT):\")\n","  print(feature_importance.head(10))\n","\n","  # === Return structured result\n","  results_saint = {\n","      'val_probs': val_probs,\n","      'test_probs': test_probs,\n","      'test_preds': test_preds,\n","      'accuracy': accuracy,\n","      'auc': auc,\n","      'best_threshold': best_threshold,\n","      'fpr': fpr,\n","      'tpr': tpr,\n","      'report_df': report_df,\n","      'feature_importance': feature_importance\n","  }\n","\n","  return model_saint, results_saint\n"],"metadata":{"id":"9NgTo3ytUV76"},"execution_count":null,"outputs":[]}]}